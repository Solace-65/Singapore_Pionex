# -*- coding: utf-8 -*-
"""Singapore_Pionex

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UaNEuZ9CrdzHZk49ONOiE8H1JTgfeheL
"""

import pandas as pd
import re

# File paths
input_file_path = '/content/241k-Singapore-pionex.com-Crypto-Trading-Bots-UsersDB-csv-2023.csv'
clean_file_path = '/content/clean_data.csv'
garbage_file_path = '/content/garbage_data.csv'
merged_clean_file_path = '/content/merged_clean_data.csv'
merged_garbage_file_path = '/content/merged_garbage_data.csv'

# Columns to remove and essential columns
columns_to_drop = ['registrationdate', 'lang', 'brandcode']
essential_columns = ['email', 'phone']
rename_columns = {'first name': 'first_name', 'last name': 'last_name'}

# Initialize DataFrames to hold clean and garbage data
merged_clean_data = pd.DataFrame()
merged_garbage_data = pd.DataFrame()

# Email validation function
def is_valid_email(email):
    email_pattern = r'^[\w\.-]+@[\w\.-]+\.\w+$'
    return bool(re.match(email_pattern, str(email)))

# Process data in chunks
chunks = pd.read_csv(input_file_path, chunksize=10000, low_memory=False)

for i, chunk in enumerate(chunks):
    # Standardize headers to lowercase and rename columns
    chunk.columns = [col.lower() for col in chunk.columns]
    chunk.rename(columns=rename_columns, inplace=True)

    # Retain specified columns in a separate copy for garbage data
    chunk_with_garbage_columns = chunk.copy()

    # Drop specified columns in the main chunk
    chunk.drop(columns=columns_to_drop, errors='ignore', inplace=True)

    # Capitalize the first letter of each name in 'first_name' and 'last_name'
    for name_col in ['first_name', 'last_name']:
        if name_col in chunk.columns:
            chunk[name_col] = chunk[name_col].fillna('').str.title()

    # Convert phone column to string, clean phone numbers by removing non-numeric characters and stripping whitespace
    chunk['phone'] = chunk['phone'].astype(str).str.strip().str.replace(r'[^0-9]', '', regex=True)

    # Validate phone numbers length (7-15 digits)
    valid_phone_mask = chunk['phone'].str.len().between(7, 15)
    invalid_phone_mask = ~valid_phone_mask

    # Validate emails
    valid_email = chunk['email'].apply(is_valid_email)

    # Separate garbage data (invalid email, invalid phone, missing essential columns, or rows with all NaN values)
    garbage_data = chunk_with_garbage_columns[~valid_email | invalid_phone_mask | chunk[essential_columns].isnull().any(axis=1)]
    garbage_data = pd.concat([garbage_data, chunk_with_garbage_columns[chunk.isnull().all(axis=1)]], ignore_index=True)

    # Filter clean data by keeping only valid rows and dropping duplicates
    clean_data = chunk[valid_email & valid_phone_mask]
    clean_data = clean_data.dropna(subset=essential_columns).drop_duplicates()

    # Append to merged data
    merged_clean_data = pd.concat([merged_clean_data, clean_data], ignore_index=True)
    merged_garbage_data = pd.concat([merged_garbage_data, garbage_data], ignore_index=True)

# Save the final cleaned and garbage data to CSV files
merged_clean_data.to_csv(clean_file_path, index=False)
merged_garbage_data.to_csv(garbage_file_path, index=False)
merged_clean_data.to_csv(merged_clean_file_path, index=False)
merged_garbage_data.to_csv(merged_garbage_file_path, index=False)

# Print the merged DataFrames
print("Cleaned data:")
print(merged_clean_data)
print("\nGarbage data:")
print(merged_garbage_data)

print("\nCleaned data saved to:", clean_file_path)
print("Garbage data saved to:", garbage_file_path)
print("Merged clean data saved to:", merged_clean_file_path)
print("Merged garbage data saved to:", merged_garbage_file_path)